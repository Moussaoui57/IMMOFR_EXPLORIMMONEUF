#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Spider

from __future__ import unicode_literals
from explorimmo.mhelpers import extract_from, get_item

import scrapy
import time
import ast
import re
from bs4 import BeautifulSoup
import json
import urlparse
import requests
import random
from datetime import datetime

from neukolln.spiders import NeukollnBaseSpider
from neukolln.items import ImmoItem
from neukolln.utils import timestamp_to_date, cast_float, clean_text, get_txt_from_element

from scrapy.utils.project import get_project_settings

UTF8_ENCODING = 'utf-8'

import sys
reload(sys)
sys.setdefaultencoding(UTF8_ENCODING)

import logging
logger = logging.getLogger()

ESTATE_TYPES = [
    "maison",
    "terrain",
    "appartement",
]

OLDNESS = ['neuf']

LOCATIONS = ['FRANCE']
COUNTRY = "FR"

# BASIC HEADER VARIABLES
HEADER_HOST = 'www.explorimmoneuf.com'
HEADER_ACCEPT_ENCODING = 'gzip, deflate, br'
HEADER_ACCEPT_LANGUAGE = 'fr-FR,fr;q=0.8,en-US;q=0.6,en;q=0.4'
HEADER_ORIGIN = 'https://www.explorimmoneuf.com'
HEADER_CONNECTION = 'keep-alive'

# IMPORTANT!
#  C'est le nombre de pages maximum autorisé pour la pagination.
# Ainsi si trop d'annonces, il faut procéder "step by step" en jouant si un critère tel que le prix
MAX_RESULTS_PER_PAGE = 18  # max returned items you can expect from the API
MAX_TOTAL_PAGINATION = 30


class ExplorimmoSpiderSpider(NeukollnBaseSpider, scrapy.Spider):
    #name = "explorimmo_spider_201809"
    #name = "explorimmo_spider_201809_V2"
    #name = "explorimmoneuf201810"
    #name = "explorimmoneuf201811"
    name="explorimmoneuf201812"
    allowed_domains = ["explorimmoneuf.com", "explorimmo.com"]

    # urls are generated by overriding the start_requests() method
    #  start_urls = ['http://explorimmoneuf.com/']

    neukolln_export_to_json = False  # default value is True
    neukolln_export_to_csv = True  # default value is False
    neukolln_export_to_tab = False  # default value is False

    def start_requests(self):
        """Generate requests here"""
        yield scrapy.Request(
            "https://www.explorimmoneuf.com/programme/immobilier-france-promoteur-1",  # only to get the min / max prices
            headers={
                "Host": HEADER_HOST,
                "Accept": "*/*",
                "Accept-Language": HEADER_ACCEPT_LANGUAGE,
                "Accept-Encoding": HEADER_ACCEPT_ENCODING,
                "Connection": HEADER_CONNECTION
            },
            method='GET',
            encoding=UTF8_ENCODING,
            dont_filter=False,  # FILTER! This url is called only once...
            callback=self.get_min_max_prices,
            # meta={
            #     #'neukolln_refresh_cache': False  # DON'T REFRESH CACHE FOR LISTING
            # }
        )

    def get_request_for_listing_page(self, priceMin, priceMax, location, currentPageNumber, oldness, estateType,
                                     callback):
        """Build URL with query parameters (price filter, etc.)"""

        # Prepare headers
        headers = {
            "Host": HEADER_HOST,
            "Accept": "application/json",
            "Accept-Language": HEADER_ACCEPT_LANGUAGE,
            "Accept-Encoding": HEADER_ACCEPT_ENCODING,
            # "Referer: http: // www.explorimmoneuf.com / logement / maison - france - page - 2?prixMin = 42000 & prixMax = 4200000
            "X-Requested-With": "XMLHttpRequest"
        }

        url = "https://www.explorimmoneuf.com/rest/programs?" \
              "location=%s" \
              "&priceMax=%s" \
              "&priceMin=%s" \
              "&sortType=13,5" \
              "&currentPageNumber=%s" \
              "&withClassifieds=full" \
              "&resultsPerPage=%d" \
              "&oldness=%s" \
              "&promoterExcluded=false" \
              "&excludeAT=false&" \
              "estateType=%s" % \
              (location,
               priceMax,
               priceMin,
               currentPageNumber,
               MAX_RESULTS_PER_PAGE,
               oldness,
               estateType)

        return scrapy.Request(
            url,
            headers=headers,
            method='GET',
            encoding=UTF8_ENCODING,
            callback=callback,
            dont_filter=True,  # DO NOT FILTER!!! => different params!!!
        )

    def get_min_max_prices(self, response):
        """Get min and max available prices"""

        # Get price_min, price_max
        priceMin = int(response.xpath('//input[@id="undefined-min"]/@value').extract_first().replace(u'\xa0', u''))
        priceMax = int(response.xpath('//input[@id="undefined-max"]/@value').extract_first().replace(u'\xa0', u''))

        # Parameters
        currentPageNumber = 1  # pagination starts at page ONE!!!

        for estateType in ESTATE_TYPES:
            for oldness in OLDNESS:
                for location in LOCATIONS:
                    yield self.get_request_for_listing_page(priceMin,
                                                            priceMax,
                                                            location,
                                                            currentPageNumber,
                                                            oldness,
                                                            estateType,
                                                            self.bisect)

    def bisect(self, response):
        """Recherche dichotomique"""

        # Load JSON data
        parsed_data = json.loads(response.text)

        # Get counter (items), current (page), total (pages)
        counter = parsed_data['counter']
        if counter and int(counter) > 0:
            current = parsed_data['pagination']['current']
            total = parsed_data['pagination']['total']

            #  Get query parameters
            queryParams = urlparse.parse_qs(urlparse.urlparse(response.url).query, True)

            # Retrieve price_min, price_max, etc.
            priceMin = int(queryParams['priceMin'][0]) if queryParams['priceMin'][0] else 0
            priceMax = int(queryParams['priceMax'][0])
            location = queryParams['location'][0]
            currentPageNumber = queryParams['currentPageNumber'][0]
            oldness = queryParams['oldness'][0]
            estateType = queryParams['estateType'][0]

            assert int(current) == int(currentPageNumber)

            if total >= MAX_TOTAL_PAGINATION:
                # interval is too big - bisect
                if priceMin >= (priceMax - 1):
                    logger.error("ERROR: price filter is not enough. Do something! "
                                 "priceMin=%s, priceMax=%s, location=%s, oldness=%s, estateType=%s, url=%s" %
                                 (priceMin, priceMax, location, oldness, estateType, response.url))
                else:
                    yield self.get_request_for_listing_page(priceMin,
                                                            priceMin + int((priceMax - priceMin) / 2),
                                                            location,
                                                            currentPageNumber,
                                                            oldness,
                                                            estateType,
                                                            self.bisect)

                    yield self.get_request_for_listing_page(priceMin + int((priceMax - priceMin) / 2) + 1,
                                                            priceMax,
                                                            location,
                                                            currentPageNumber,
                                                            oldness,
                                                            estateType,
                                                            self.bisect)

            else:
                for page in range(int(currentPageNumber), total + 1,
                                  1):  # for i in 1 to total! pagination starts at page ONE!
                    yield self.get_request_for_listing_page(priceMin,
                                                            priceMax,
                                                            location,
                                                            page,
                                                            oldness,
                                                            estateType,
                                                            self.parse_listing_page)

    def get_agence_tel(self, id_client, referer):
        """Return the phone number of the contact [UTILS]"""
        url = "https://www.explorimmo.com/rest/programs/%s/phone" % str(id_client)

        # Prepare headers
        headers = {
            'Host': 'www.explorimmo.com',
            #  'User-Agent': HEADER_USER_AGENT,
            "Accept": "application/json",
            'Accept-Encoding': HEADER_ACCEPT_ENCODING,
            'Accept-Language': HEADER_ACCEPT_LANGUAGE,
            'Referer': referer,
            'X-Requested-With': 'XMLHttpRequest',
            'Connection': HEADER_CONNECTION,
        }

        try:
            # Return request
            response = requests.session().get(
                url,
                headers=headers,
            )
            if response.status_code == 200:
                # Load JSON data
                phone_json_data = json.loads(response.content)

                # Fill phone
                return phone_json_data.get('phoneNumber') if phone_json_data else None
        except Exception, e:
            logger.error("ERROR while parsing phone: %s" % str(e))

    def get_url_to_program(self, _id):
        return "https://www.explorimmoneuf.com/programme/detail-{id}".format(id=_id)

    def parse_listing_page(self, response):
        """
        Crawl and parse the first page from listing [LISTING MODE]
        Example:
            https://www.explorimmoneuf.com/rest/classifieds?location=France&priceMax=&priceMin=&sortType=13,5&currentPageNumber=2&withClassifieds=full&resultsPerPage=50&oldness=neuf&transaction=vente&promoterExcluded=true&excludeAT=false&estateType=maison
        """

        # Load JSON data
        parsed_data = json.loads(response.text)

        # Iterate over elements
        # logger.debug('[PARSE_LISTING_PAGE] programs=%s' % parsed_data['programs'])
        for el in parsed_data['programs']:

            # Instantiate the item
            item = ImmoItem()

            if el.get('isProgram', False):
                # Only program

                # ID
                item['ID_CLIENT'] = el['id']

                # ANNONCE LINK
                item['ANNONCE_LINK'] = self.get_url_to_program(item['ID_CLIENT'])
                item['PAYS_AD'] = 'FR'
                item['SELLER_TYPE'] = 'pro'

                item['NEUF_IND'] = "Y"  # NEUF!!!
                item['ACHAT_LOC'] = "1"  # A VENDRE!!!
                item['FROM_SITE'] = "http://www.explorimmoneuf.com"
                item['NOM'] = el.get('name')
                item['MINI_SITE_ID'] = el.get('customerId')
                item['DEPARTEMENT'] = el.get('departmentCode')
                #item['PHOTO'] = el.get('photoCount')
                item['CATEGORIE'] = el.get('estateType')
                #item['STOCK_NEUF'] = el.get('accommodationsCount')
                city = re.search(r'(.*) \(\d{2}\)', el.get('localisation', ''))
                if city and city.groups():
                    #item['VILLE'] = city.groups()[0]
                    item['VILLE'] = city.groups()
                else:
                    item['VILLE'] = el.get('localisation')
                item['ANNONCE_DATE'] = el.get('modificationDate')
                if item['ANNONCE_DATE']:
                    item['ANNONCE_DATE'] = datetime.strptime(item['ANNONCE_DATE'], "%d/%m/%Y").strftime("%Y-%m-%d %H:%M:%S")

                item['SOLD'] = "Y"
                if el.get('isAvailable'):
                    if (el['isAvailable'] == True) or (el['isAvailable'] == "true"):
                        item['SOLD'] = "N"

                item['CP'] = el.get('postalCode')
                # item['PRIX'] = el.get('priceMin')

                item['SURFACE_TERRAIN'] = None
                item['M2_TOTALE'] = None
                if "Terrain" in item['CATEGORIE']:
                    item['SURFACE_TERRAIN'] = el.get('areaMin')
                else:
                    item['M2_TOTALE'] = el.get('areaMin')

                #item['PIECE'] = el.get('roomCountMin')
                item['LATITUDE'] = el.get('latitude')
                item['LONGITUDE'] = el.get('longitude')

                item['NEUKOLLN_DEFAULT_CC_AGENCE_TEL'] = COUNTRY
                item['NEUKOLLN_ORIGINAL_PHONE_AGENCE_TEL'] = self.get_agence_tel(item['ID_CLIENT'], item['ANNONCE_LINK'])

                yield scrapy.Request(item['ANNONCE_LINK'],
                                     headers={
                                         'Host': 'www.explorimmoneuf.com',
                                         #  'User-Agent': HEADER_USER_AGENT,
                                         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                         'Accept-Encoding': HEADER_ACCEPT_ENCODING,
                                         'Accept-Language': HEADER_ACCEPT_LANGUAGE,
                                         'Referer': str(response.url),
                                         'Connection': HEADER_CONNECTION,
                                         'Upgrade-Insecure-Requests': '1',
                                         'Cache-Control': "max-age=0"
                                     },
                                     method='GET',
                                     encoding=UTF8_ENCODING,
                                     callback=self.parse_item,
                                     errback=lambda failure, item=item: self.on_error(failure, item),
                                     dont_filter=False,  # FILTER!!!
                                     meta={
                                         'item': str(item),
                                     })

    def parse_item(self, response):
        """
        Parse data from details page [DETAILS MODE]
        """

        # Retieve item
        item = response.meta['item']

        # Convert returned string to dictionary
        item = ast.literal_eval(item)

        # MIN PRICE

        #if not item.get('PRIX'):
            #min_prix = response.xpath('//span[@class="price"]//text()')
            #if min_prix:
                #item['PRIX'] = min_prix.extract_first().replace(u'\xa0', '').replace(u'\u20ac', '').replace(' ', '')
            #if not item.get('PRIX') or item['PRIX'] == 0:
                # from JS data
                #if re.search(r'"price":([^,]*),', response.text) and re.search(r'"price":([^,]*),', response.text).groups():
                    #item['PRIX'] = re.search(r'"price":([^,]*),', response.text).groups()[0]


	price = re.search('"priceMin":([^,]+),', response.body).groups()[0]
	if price:
		item['PRIX'] = price

        # Sometimes, there are more categories...
        #item['CATEGORIE'] = ", ".join([x.title() for x in list(set(response.xpath('//table[@class="list-accommodations"]//tbody//tr//td[@data-name="Type"]//text()').extract()))])

        category = re.search('.*,"estateType":"([^"]+)".*', response.text)
        if category and category.groups():
            item['CATEGORIE'] = category.groups()[0].title()


        # STOCK_NEUF
        #rows = response.xpath('//div[@class="list-accommodations"]//div[contains(@class, "row-tn")]')
        #if rows:
            #item['STOCK_NEUF'] = len(rows.extract())

	item['STOCK_NEUF'] = response.text.count(',{"id":')+1

        # DESCRIPTION
        description = response.xpath('//div[@id="js-description-content"]//text()')
        if description:
            item['DESCRIPTION'] = " ".join(description.extract())

        # NOM
        nom = response.xpath('//div[@class="front-head"]//text()')
        if nom:
            item['NOM'] = clean_text(" ".join(nom.extract()))

        # PIECE
        #if not item.get('PIECE'):
        #piece = response.xpath('//span[contains(@class, "circle-tn")]')
        #if piece:
            #piece = piece.extract()
            #item['PIECE'] = min(map(int, [re.search(".*circle-t(\d+).*", x).groups()[0] for x in piece]))

        #piece =response.xpath('//div[@class="second-line"]/text()').extract_first()
	
	#p1 = extract_from(piece, '.*  .* (.*) .*')
	#p2 = extract_from(piece, '.*  (.*) .*')
	#p3 = extract_from(piece, '.*  .* (.*) .*')
	#if p1:
            #item['PIECE'] = p1
        #elif p2:
            #item['PIECE'] = p2
        #elif p3:
            #item['PIECE'] = p3
	#else:
	    #item['PIECE'] = ""

	#piece1 = re.search('"roomCountMin":([^,]+),', response.body).groups()[0]
	#piece2 = re.search('"roomCount":([^,]+),', response.body).groups()[0]
	#piece1 = re.findall('"roomCountMin":(.*?),',response.text)[0]
	piece1 = re.findall('"roomCountMin":(.*?),',response.text)
	if piece1:
		nb=piece1[0]
	else:
		nb=''
	item["PIECE"]=nb
	
        # M2
        if not item.get('SURFACE_TERRAIN') and not item.get('M2_TOTALE'):
            m2 = response.xpath('//td[@data-name="Surface"]//text()')
            itemM2 = None
            if m2:
                m2 = m2.extract()
                itemM2 == min(map(float, [re.search('(\d+\.?\d*) .*', x).groups()[0] if re.search('(\d+\.?\d*) .*', x) else 0 for x in m2]))
                itemM2 = cast_float(itemM2, 2)
            if (not itemM2) or str(itemM2) == "0.00":
                # from JS data
                if re.search(r'"area":([^,]*),', response.text) and re.search(r'"area":([^,]*),', response.text).groups():
                    itemM2 = re.search(r'"area":([^,]*),', response.text).groups()[0]
                    itemM2 = cast_float(itemM2, 2)

            if itemM2:
                item['M2'] = cast_float(itemM2, 2)

                if "Terrain" in item['CATEGORIE']:
                    item['SURFACE_TERRAIN'] = cast_float(itemM2, 2)
                else:
                    #item['M2_TOTALE'] = cast_float(itemM2, 2)
                    item['M2_TOTALE'] = cast_float(itemM2, 0)

        # floor:
        # floor = response.xpath('//td[@class="td-floor"]//text()')
        # if floor:
        #     floor = floor.etxract()
        #     floor = [x if not "Rez-de-" in x else 0 for x in floor]
        #     item['ETAGE'] = min(map(int, floor))
        #     item['ETAGE'] = min(map(int, floor))


        # PHOTO
	
        #photo = response.xpath('//div[contains(@class, "slick-slide slick-active")]')
        #if photo:
            #photo = photo.extract()
            #item['PHOTO'] = len(photo)
        #else:
            #item['PHOTO'] = 0
	

	photo = response.text.count(',"overlayUrl":')
	if photo:
		item['PHOTO'] = photo

        # OPTIONS
        options = re.search(r'"options":\[([^\]]+)\]', response.text)
        if options and options.groups():
            options = options.groups()[0]
            if "interphone" in options:
                item['INTERPHONE'] = 'Y'
            else:
                item['INTERPHONE'] = 'N'

            if "digicode" in options:
                item['DIGICODE'] = 'Y'
            else:
                item['DIGICODE'] = 'N'

            if "ascenseur" in options:
                item['ASCENSEUR'] = 'Y'
            else:
                item['ASCENSEUR'] = 'N'

            if "parking" in options:
                item['PARKING'] = 'Y'

            if "piscine" in options:
                item['PISCINE'] = 'Y'

        # CREATION DATE
        # creation_date = re.search(r'"creationDate":"(\S+)",', response.text)
        # if creation_date and creation_date.group():
        #     creation_date = creation_date.group()

        # ADRESSE
        if not item.get('ADRESSE'):
            address = re.search(r'"address":"([^"]*)",', response.text)
            if address and address.groups():
                item['ADRESSE'] = address.groups()[0]

        # REGION
        region = re.search(r'"region":"([^"]*)",', response.text)
        if region and region.groups():
            item['REGION'] = region.groups()[0]

        # AGENCE NOM
        # agency_name = re.search(r'"agency_name":"([^"]*)",', response.text)
        agencyName = re.search(r'"agencyName":"([^"]*)",', response.text)
        if agencyName and agencyName.groups():
            item['AGENCE_NOM'] = agencyName.groups()[0]

        # Get id to the first element to get more info from the detail page
        element_id = re.search(r'"accommodations":\[\{"id":"([^"]*)",', response.text)
        if element_id and element_id.group():
            element_id = element_id.groups()[0]

            # Crawl detail page to have more info
            url = "https://www.explorimmo.com/annonce-%s.html" % str(element_id)
            yield scrapy.Request(url,
                                 headers={
                                     'Host': 'www.explorimmo.com',
                                     #  'User-Agent': HEADER_USER_AGENT,
                                     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                                     'Accept-Encoding': HEADER_ACCEPT_ENCODING,
                                     'Accept-Language': HEADER_ACCEPT_LANGUAGE,
                                     'Referer': str(response.url),
                                     'Connection': HEADER_CONNECTION,
                                     'Upgrade-Insecure-Requests': '1',
                                     'Cache-Control': "max-age=0"
                                 },
                                 method='GET',
                                 encoding=UTF8_ENCODING,
                                 callback=self.parse_lot,
                                 errback=lambda failure, item=item: self.on_error(failure, item),
                                 dont_filter=False,  # DO NOT FILTER
                                 meta={
                                     'item': str(item),
                                 })

        else:
            if not item.get('CP') and item.get('DEPARTEMENT') and len("".join([el for el in item['DEPARTEMENT'] if el.isdigit()])) == 2:
                item['CP'] = item['DEPARTEMENT'] + "000"
            yield item

    def get_agence_website(self, url, referer):
        # Prepare headers
        headers = {
            'Host': 'www.explorimmo.com',
            #  'User-Agent': HEADER_USER_AGENT,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            'Accept-Encoding': HEADER_ACCEPT_ENCODING,
            'Accept-Language': HEADER_ACCEPT_LANGUAGE,
            'Referer': referer,
            'Connection': HEADER_CONNECTION,
            'Upgrade-Insecure-Requests': "1",
        }

        try:
            # Return request
            response = requests.session().get(
                url,
                headers=headers,
            )
            if response.status_code == 200:
                website = None
                if re.search('urlClient=([^"]+)', response.text) and re.search('urlClient=([^"]+)', response.text).groups():
                    website = re.search('urlClient=([^"]+)', response.content).groups()[0]
                return True, website
        except Exception, e:
            logger.error("ERROR while parsing website: %s" % str(e))

    def parse_lot(self, response):
        """
        Parse data from lot [DETAILS MODE]
        Example:
            https://www.explorimmoneuf.com/logement/detail-33689517-1
            ou
            https://www.explorimmo.com/annonce-33689658-1.html
        """

        # Retieve item
        item = response.meta['item']

        # Convert returned string to dictionary
        item = ast.literal_eval(item)

        # Get agency location
        agency_location = response.xpath('//span[contains(@class,"agency-location")]/text()')
        if agency_location and len(agency_location) > 0:
            agency_location = clean_text(" ".join(agency_location.extract()))

            reg = re.compile('^.*(?P<zipcode>\d{5}).*$')
            match = reg.match(agency_location)

            if match:
                # there is a zip code
                cp = match.groupdict()['zipcode']
                item['AGENCE_CP'] = cp
                item['AGENCE_DEPARTEMENT'] = cp[:0]  # first two digits

                arr = agency_location.split(item['AGENCE_CP'])
                if arr and len(arr) > 0:
                    item['AGENCE_VILLE'] = clean_text(arr[-1]).upper()

                if arr and len(arr) > 1:
                    item['AGENCE_ADRESSE'] = clean_text(arr[0])

            else:
                # put everything in this single field
                item['AGENCE_ADRESSE'] = agency_location

        # Get agence name
        if not item.get('AGENCE_NOM'):
            agence_name = response.xpath('//span[contains(@class,"agency-name")]/text()')
            if agence_name and len(agence_name) > 0:
                agence_name = clean_text(" ".join(agence_name.extract()))
                item['AGENCE_NOM'] = agence_name if (agence_name and not item.get('AGENCE_NOM')) else item['AGENCE_NOM']

        # Get agence website
        website = response.xpath('//a[contains(@class,"inquiry-agency-link idLienSite")]/@href')
        if website:
            item['WEBSITE'] = website.extract()

        # Get agence minisite
        minisite = response.xpath('//a[contains(@class,"inquiry-agency-link js-inquiry-agency-link")]/@href')
        if item.get('MINI_SITE_ID'):
            url = "https:/www.explorimmo.com/agence+immobiliere-%s-tous-bien.html" % str(item['MINI_SITE_ID'])
            res = self.get_agence_website(url, response.url)
            if res and len(res) == 2:
                item['MINI_SITE_URL'] = url
                item['WEBSITE'] = res[1]
        elif minisite:
            minisite = minisite.extract()
            item['MINI_SITE_URL'] = "https://www.explorimmo.com/%s" % clean_text(minisite[0]) if minisite else None

        if not item.get('CP') and item.get('DEPARTEMENT') and len("".join([el for el in item['DEPARTEMENT'] if el.isdigit()])) == 2:
            item['CP'] = item['DEPARTEMENT'] + "000"

        yield item

